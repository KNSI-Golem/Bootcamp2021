{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "import warnings\r\n",
    "from sklearn.exceptions import UndefinedMetricWarning\r\n",
    "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic regression\r\n",
    "Sources: \r\n",
    "- [sklearn model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\r\n",
    "- [wiki](https://en.wikipedia.org/wiki/Logistic_regression)\r\n",
    "- [MLMastery](https://machinelearningmastery.com/logistic-regression-for-machine-learning/)\r\n",
    "\r\n",
    "## Simple example\r\n",
    "Let's generate dataset first"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gen_example(lb: int, size: int, mult: int) -> pd.DataFrame:\r\n",
    "    \"\"\"\r\n",
    "    Generates simple example DataFrame with x ft and y target\r\n",
    "    Parameters:\r\n",
    "    lb (int): lower boundary of x\r\n",
    "    size (int): category size (dataset size = 2*size)\r\n",
    "    mult (int): multiplier\r\n",
    "    Returns:\r\n",
    "    X (pd.DataFrame): dataset\r\n",
    "    \"\"\"\r\n",
    "    x_values = np.concatenate((np.random.rand(size)*mult-lb, \\\r\n",
    "        np.random.rand(size)*mult), axis=0)    # array\r\n",
    "    y_values = np.array([0]*size+[1]*size)  # labels array\r\n",
    "    X = pd.DataFrame({'x': x_values, 'y': y_values})    # DataFrame\r\n",
    "    return X\r\n",
    "\r\n",
    "def plot_dataset(dataset: pd.DataFrame) -> None:\r\n",
    "    \"\"\"\r\n",
    "    Plots dataset\r\n",
    "    Parameters:\r\n",
    "    dataset (pd.DataFrame): input DataFrame\r\n",
    "    \"\"\"\r\n",
    "    plt.scatter(dataset.loc[dataset.y==0, 'x'], dataset.loc[dataset.y==0, 'y'], label=0)    # scatter first class\r\n",
    "    plt.scatter(dataset.loc[dataset.y==1, 'x'], dataset.loc[dataset.y==1, 'y'], label=1)    # scatter second class\r\n",
    "    plt.legend()\r\n",
    "    plt.show()   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lb, size, mult = 15, 15, 10\r\n",
    "X = gen_example(lb, size, mult)\r\n",
    "X.sample(7) # sample data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization\r\n",
    "Plot the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_dataset(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lb, size, mult = 7, 20, 10  # Keep calm and play around with different parameters :D\r\n",
    "train_set = gen_example(lb, size, mult)\r\n",
    "test_set = gen_example(lb, size//2, mult)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_dataset(train_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_dataset(test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I kept getting annoyed by all this `dataset.x.values.reshape(-1, 1)` stuff so here are some cool functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_x(dataset, target='y'):\r\n",
    "    \"\"\"\r\n",
    "    Fixes annoying dimensionality issue with LogisticRegression\r\n",
    "    \"\"\"\r\n",
    "    if isinstance(dataset, np.ndarray) and len(dataset.shape) == 1:\r\n",
    "        return dataset.reshape(-1, 1)\r\n",
    "    if isinstance(dataset, pd.Series):\r\n",
    "        return dataset.values.reshape(-1, 1)\r\n",
    "    cols = [col for col in dataset.columns if col != target]\r\n",
    "    if len(cols) == 1:\r\n",
    "        return dataset.loc[:, cols].values.reshape(-1, 1)\r\n",
    "    return dataset.loc[:, cols]\r\n",
    "\r\n",
    "def get_y(dataset, target='y'):\r\n",
    "    return dataset.loc[:, target]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = LogisticRegression()    # define the model\r\n",
    "model.fit(get_x(train_set, 'y'), train_set.y)   # fit model\r\n",
    "X_test = np.linspace(-lb, mult, 1000)   # logistic regression plot x values\r\n",
    "y_pred = model.predict_proba(get_x(X_test))[:, 1]   # model output\r\n",
    "# here we use model.predict_proba instead of model.predict to get probabilities\r\n",
    "# at every point in X_test array instead of [0, 1]^(size*2) vector\r\n",
    "_, axs = plt.subplots(1, 2, figsize=(10, 5))    # this trick enables us to plot two charts at once\r\n",
    "for idx, dataset in enumerate((train_set, test_set)):\r\n",
    "    # axs is just an array of ax objects, thus we can access them via a standard indexing\r\n",
    "    axs[idx].plot(X_test, y_pred, color='red', label='probs')    # plot the curve\r\n",
    "    axs[idx].scatter(dataset.loc[dataset.y==0, 'x'], dataset.loc[dataset.y==0, 'y'], label=0)\r\n",
    "    axs[idx].scatter(dataset.loc[dataset.y==1, 'x'], dataset.loc[dataset.y==1, 'y'], label=1)\r\n",
    "    # and the data\r\n",
    "    axs[idx].legend()   # show the labels\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Once we've got all of this covered let'c compile it into one function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def do_everything(lb: int, size: int, mult: int, model) -> tuple:\r\n",
    "    \"\"\"\r\n",
    "    Generates example train and test DataFrames with x ft and y target\r\n",
    "    Plots them\r\n",
    "    Trains model and returns datasets and model\r\n",
    "    Parameters:\r\n",
    "    lb (int): lower boundary of x\r\n",
    "    size (int): category size (dataset size = 2*size)\r\n",
    "    mult (int): multiplier\r\n",
    "    model: ML model instance\r\n",
    "    Returns:\r\n",
    "    train_set (pd.DataFrame), test_set (pd.DataFrame), model: datasets, trained model\r\n",
    "    \"\"\"\r\n",
    "    train_set = gen_example(lb, size, mult)\r\n",
    "    test_set = gen_example(lb, size//2, mult)\r\n",
    "    print('train_set')\r\n",
    "    plot_dataset(train_set)\r\n",
    "    print('test_set')\r\n",
    "    plot_dataset(test_set)\r\n",
    "    model.fit(get_x(train_set, 'y'), train_set.y)\r\n",
    "    X_test = np.linspace(-lb, mult, 1000)\r\n",
    "    y_pred = model.predict_proba(get_x(X_test))[:, 1]\r\n",
    "    _, axs = plt.subplots(1, 2, figsize=(10, 5))\r\n",
    "    for idx, dataset in enumerate((train_set, test_set)):\r\n",
    "        axs[idx].plot(X_test, y_pred, color='red', label='probs')\r\n",
    "        axs[idx].scatter(dataset.loc[dataset.y==0, 'x'], dataset.loc[dataset.y==0, 'y'], label=0)\r\n",
    "        axs[idx].scatter(dataset.loc[dataset.y==1, 'x'], dataset.loc[dataset.y==1, 'y'], label=1)\r\n",
    "        axs[idx].legend()\r\n",
    "    plt.show()\r\n",
    "    return train_set, test_set, model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Confusion Matrix\r\n",
    "Source: \r\n",
    "- [Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\r\n",
    "- [Heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\r\n",
    "- [CM How-To](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)\r\n",
    "\r\n",
    "Annotations:\r\n",
    "- TN = True Negative (true value is 0 and model predicts 0)\r\n",
    "- FN = False Negative (true value is 1 and model predicts 0)\r\n",
    "- TP = True Positive (true value is 1 and model predicts 1)\r\n",
    "- FP = False Positive (true value is 0 and model predicts 1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from seaborn import heatmap\r\n",
    "heatmap([[0, 1], [2, 3]], annot=[['TN', 'FP'], ['FN', 'TP']], fmt='')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\r\n",
    "y_pred_train = model.predict(get_x(train_set, 'y')) # predicted labels array (train_set)\r\n",
    "y_pred_test = model.predict(get_x(test_set, 'y'))   # predicted labels array (test_set)\r\n",
    "cm_train = confusion_matrix(train_set.y, y_pred_train)  # confusion matrix train_set\r\n",
    "cm_test = confusion_matrix(test_set.y, y_pred_test)     # confusion matrix test_set\r\n",
    "cm_train, cm_test   # print"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(10, 4))    # same trick as before\r\n",
    "heatmap(cm_train, annot=True, ax=axs[0])    # we can pass ax object as a parameter to the heatmap\r\n",
    "heatmap(cm_test, annot=True, ax=axs[1])     # to plot it on a dedicated subplot\r\n",
    "axs[0].set_title('train cm')    # setting title\r\n",
    "axs[1].set_title('test cm')\r\n",
    "for idx in (0, 1):\r\n",
    "    axs[idx].set_xlabel('prediction')\r\n",
    "    axs[idx].set_ylabel('true label')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metrics\r\n",
    "\r\n",
    "## The most important ones: [wiki](https://en.wikipedia.org/wiki/Precision_and_recall)\r\n",
    "- [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) = (TP+TN)/(TP+TN+FP+FN)\r\n",
    "- [precision](https://www.google.com/search?q=precision+sklearn&oq=precision+sklearn&aqs=chrome..69i57j0i22i30l9.1827j0j4&sourceid=chrome&ie=UTF-8) = TP/(TP+FP)\r\n",
    "- [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) = TP/(TP+FN)\r\n",
    "- [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) = 2*(precision*recall)/(precision+recall)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\r\n",
    "y_pred_train = model.predict(get_x(train_set, 'y'))\r\n",
    "y_pred_test = model.predict(get_x(test_set, 'y'))\r\n",
    "print('Training set')\r\n",
    "print('Accuracy:', accuracy_score(test_set.y, y_pred_test))\r\n",
    "print('Precision:', precision_score(test_set.y, y_pred_test))\r\n",
    "print('Recall:', recall_score(test_set.y, y_pred_test))\r\n",
    "print('F1 score:', f1_score(test_set.y, y_pred_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Once again compilation function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def show_metrics(model, dataset, target='y'):\r\n",
    "    \"\"\"\r\n",
    "    Prints out the most important metrics\r\n",
    "    \"\"\"\r\n",
    "    y_pred = model.predict(get_x(dataset, target))\r\n",
    "    heatmap(confusion_matrix(get_y(dataset, target), y_pred), annot=True, fmt='d')\r\n",
    "    plt.show()\r\n",
    "    print('Accuracy:', accuracy_score(get_y(dataset, target), y_pred))\r\n",
    "    print('Precision:', precision_score(get_y(dataset, target), y_pred, average='weighted'))\r\n",
    "    print('Recall:', recall_score(get_y(dataset, target), y_pred, average='weighted'))\r\n",
    "    print('F1 score:', f1_score(get_y(dataset, target), y_pred, average='weighted'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The less-obvious ones: \r\n",
    "- [ROC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\r\n",
    "- [AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)\r\n",
    "- [Plotting the ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html)\r\n",
    "- [More info](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\r\n",
    "\r\n",
    "To show these metrics at their best we need bigger datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lb, size, mult = 80, 100, 100   # bigger dataset\r\n",
    "model = LogisticRegression()\r\n",
    "train_set, test_set, model = do_everything(lb, size, mult, model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "show_metrics(model, train_set), show_metrics(model, test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score\r\n",
    "RocCurveDisplay.from_estimator(model, get_x(train_set, 'y'), train_set.y)   # roc curve\r\n",
    "plt.show()\r\n",
    "y_true = train_set.y\r\n",
    "y_proba = model.predict_proba(get_x(train_set, 'y'))[:, 1]  # probabilites output\r\n",
    "print('Area Under the ROC Curve score:', roc_auc_score(y_true, y_proba))    # auc score\r\n",
    "RocCurveDisplay.from_estimator(model, get_x(test_set, 'y'), test_set.y)\r\n",
    "y_true = test_set.y\r\n",
    "y_proba = model.predict_proba(get_x(test_set, 'y'))[:, 1]\r\n",
    "plt.show()\r\n",
    "print('Area Under the ROC Curve score:', roc_auc_score(y_true, y_proba))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# But why all of the above even matter? Imbalanced datasets case study"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's implement a pretty useless model\r\n",
    "It literally does nothing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DumbModel():\r\n",
    "    \"\"\"\r\n",
    "    Example class to show how does the imbalance issue work\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self):\r\n",
    "        self._estimator_type = 'classifier'\r\n",
    "        self.classes_ = [0, 1]\r\n",
    "        print('DumbModel()')    # just to mimic LogisticRegression\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        pass    # xD\r\n",
    "\r\n",
    "    def predict(self, X):\r\n",
    "        return np.zeros(X.shape[0])\r\n",
    "    \r\n",
    "    def predict_proba(self, X):\r\n",
    "        return np.array([[1, 0]]*X.shape[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dumb_model = DumbModel()\r\n",
    "dumb_model.fit(get_x(train_set, 'y'), train_set.y)  # yeah, \"\"fitting\"\"\r\n",
    "print('DumbModel\\'s predictions:', dumb_model.predict(get_x(test_set, 'y')[45:55]))\r\n",
    "print('A proper model\\'s predictions:', model.predict(get_x(test_set, 'y')[45:55]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_set, test_set, dumb_model = do_everything(lb, size, mult, dumb_model) # performance on a similar dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "show_metrics(dumb_model, train_set) # metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pretty stupid, isn't it?\r\n",
    "## But then, let's create a new dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_imbalanced_sample(size, mult, lb, ratio):\r\n",
    "    \"\"\"\r\n",
    "    Generates a dataset with a defined ratio between True and False labels\r\n",
    "    \"\"\"\r\n",
    "    new_x = np.random.rand(size)*mult-lb\r\n",
    "    new_y = (new_x >= sorted(new_x)[-int(ratio*size)])|(np.random.rand(size) < ratio/5)\r\n",
    "    imbalanced_dataset = pd.DataFrame({'x': new_x, 'y': new_y})\r\n",
    "    return imbalanced_dataset\r\n",
    "\r\n",
    "imbalanced_dataset = get_imbalanced_sample(400, 15, 10, 0.02)\r\n",
    "dumb_model = DumbModel()\r\n",
    "plot_dataset(imbalanced_dataset)    # nice"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## And use our DumbModel to run predictions on it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split    # finally a proper approach :D\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(imbalanced_dataset.x, imbalanced_dataset.y, test_size=0.5)\r\n",
    "dumb_model.fit(X_train, y_train)\r\n",
    "model.fit(get_x(X_train), y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## And score it with just an accuracy score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = dumb_model.predict(X_test)\r\n",
    "print('DumbModel accuracy score:', accuracy_score(y_test, y_pred))\r\n",
    "y_pred = model.predict(get_x(X_test))\r\n",
    "print('LogisticRegression accuracy score:', accuracy_score(y_test, y_pred))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Surprise surprise!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's now behave like pro and check the other metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_dataset = pd.DataFrame({'x': X_test, 'y': y_test})\r\n",
    "show_metrics(dumb_model, test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "RocCurveDisplay.from_estimator(dumb_model, get_x(test_dataset, 'y'), test_dataset.y)\r\n",
    "plt.show()\r\n",
    "y_true = test_dataset.y\r\n",
    "y_proba = dumb_model.predict_proba(get_x(test_dataset, 'y'))[:, 1]\r\n",
    "print('Area Under the ROC Curve score:', roc_auc_score(y_true, y_proba))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What appears to have happened? :D\r\n",
    "((protip: accuracy metric for this case is  *`u s e l e s s`*))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's compile all of this into a one function again, now featuring comparison between models' performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compare_models(model_one, model_two, dataset, target='y'):\r\n",
    "    def calculate_metrics(model, dataset, model_name, dataset_name):\r\n",
    "        # util func returning a DF with metrics wrt model, dataset\r\n",
    "        y_pred = model.predict(get_x(dataset, target))\r\n",
    "        y_true = get_y(dataset, target)\r\n",
    "        scores = [accuracy_score(y_true, y_pred), precision_score(y_true, y_pred, average='weighted'), \\\r\n",
    "            recall_score(y_true, y_pred, average='weighted'), f1_score(y_true, y_pred, average='weighted')]\r\n",
    "        return pd.DataFrame({'model': [model_name]*4, 'dataset': [dataset_name]*4,\\\r\n",
    "            'metric': ['accuracy', 'precision', 'recall', 'f1'], 'value': scores})\r\n",
    "\r\n",
    "    def plot_datasets(train, test):\r\n",
    "        # util func for logistic reg dataset plots\r\n",
    "        print('train set')\r\n",
    "        plot_dataset(train)\r\n",
    "        print('test set')\r\n",
    "        plot_dataset(test)\r\n",
    "    \r\n",
    "    def plot_probas(ds, model, axs):\r\n",
    "        # util func for logistic reg line plots\r\n",
    "        axs[y_idx][x_idx].scatter(ds.loc[ds.y==0, 'x'].values, ds.loc[ds.y==0, 'y'].values, label=0)\r\n",
    "        axs[y_idx][x_idx].scatter(ds.loc[ds.y==1, 'x'].values, ds.loc[ds.y==1, 'y'].values, label=1)\r\n",
    "        x = np.linspace(ds.x.min(), ds.x.max(), 1000)\r\n",
    "        y_score = model.predict_proba(get_x(x))[:, 1]\r\n",
    "        axs[y_idx][x_idx].plot(x, y_score, color='red', label='probs')\r\n",
    "        axs[y_idx][x_idx].legend()\r\n",
    "        axs[y_idx][x_idx].set_title(model_name+' on '+ds_name)\r\n",
    "    \r\n",
    "    def plot_heatmaps(model, ds, ht_axs):\r\n",
    "        # util func for heatmap plotting\r\n",
    "        y_pred = model.predict(get_x(ds, target))\r\n",
    "        heatmap(confusion_matrix(get_y(ds, target), y_pred), annot=True, ax=ht_axs[y_idx][x_idx], fmt='d')\r\n",
    "        ht_axs[y_idx][x_idx].set_title(model_name+' on '+ds_name)\r\n",
    "\r\n",
    "    is_log_reg = True if target=='y' else False # in a couple of minutes we'll see why this is crucial\r\n",
    "    cols = [col for col in dataset.columns if col!=target]  # same here\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset.loc[:, cols], get_y(dataset, target), test_size=0.4, random_state=2664)\r\n",
    "    train_dataset = pd.concat((X_train, y_train), axis=1)\r\n",
    "    test_dataset = pd.concat((X_test, y_test), axis=1)\r\n",
    "    model_one.fit(get_x(X_train, target), y_train)\r\n",
    "    model_two.fit(get_x(X_train, target), y_train)\r\n",
    "    if is_log_reg:  # here we go\r\n",
    "        plot_datasets(train_dataset, test_dataset)\r\n",
    "        _, axs = plt.subplots(2, 2, figsize=(10, 10))   # probabilities plots per model, dataset\r\n",
    "        _, roc_axs = plt.subplots(2, 2, figsize=(10, 10))   # ROC curve per model, dataset\r\n",
    "    _, ht_axs = plt.subplots(2, 2, figsize=(10, 10))   # heatmaps per model, dataset\r\n",
    "    metrics_df = pd.DataFrame(columns=['model', 'dataset', 'metric', 'value'])\r\n",
    "    print('Models predictions')\r\n",
    "    # iterate over datasets\r\n",
    "    for y_idx, d in enumerate(((train_dataset, 'train'), (test_dataset, 'test'))):\r\n",
    "        ds, ds_name = d\r\n",
    "        # and models\r\n",
    "        for x_idx, m in enumerate(((model_one, 'one'), (model_two, 'two'))):\r\n",
    "            model, model_name = m\r\n",
    "            if is_log_reg:\r\n",
    "                # -------probabilities section----------\r\n",
    "                plot_probas(ds, model, axs)\r\n",
    "                # -------ROC curves section--------\r\n",
    "                RocCurveDisplay.from_estimator(model, get_x(ds, target), ds.y, ax=roc_axs[y_idx][x_idx])\r\n",
    "            # -------heatmaps section-----------\r\n",
    "            plot_heatmaps(model, ds, ht_axs)\r\n",
    "            metrics_df = pd.concat((metrics_df, calculate_metrics(model, ds, model_name, ds_name)))\r\n",
    "    _, axs = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\r\n",
    "    for idx, ds_name in enumerate(('train', 'test')):\r\n",
    "        # yeah, I know it's magic, but digging down into pandas \r\n",
    "        # can reveal what on earth is going on in here xD\r\n",
    "        metrics_df.loc[metrics_df.dataset==ds_name, :].groupby(['model', 'metric']).value.agg(lambda x: x)\\\r\n",
    "            .unstack(level=0).plot(kind='bar', subplots=False, ax=axs[idx], title=ds_name)\r\n",
    "        # (((worth it)))\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "compare_models(LogisticRegression(random_state=1000), dumb_model, imbalanced_dataset)   # as simple as that"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "compare_models(LogisticRegression(class_weight='balanced', random_state=1000), LogisticRegression(random_state=1000), imbalanced_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "another_imbalanced_dataset = get_imbalanced_sample(200, 100, 100, 0.40) # let's try something else\r\n",
    "compare_models(LogisticRegression(class_weight='balanced', random_state=1000), LogisticRegression(random_state=1000), another_imbalanced_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-Class Classification and the overfitting and underfitting problem"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.datasets import load_iris  # a classic\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "data = load_iris(as_frame=True)\r\n",
    "# we need to add some noise to the dataset, bcoz it's too clean and too good\r\n",
    "X = pd.DataFrame(data.data+np.random.rand(*data.data.shape)*4, columns=data.feature_names)\r\n",
    "y = pd.DataFrame(data.target.T, columns=['target'])\r\n",
    "df = pd.concat((X, y), axis=1)\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y.value_counts()    # 3 classes!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75)\r\n",
    "train_df = pd.concat((X_train, y_train), axis=1)\r\n",
    "test_df = pd.concat((X_test, y_test), axis=1)\r\n",
    "model = DecisionTreeClassifier(max_depth=4)\r\n",
    "model.fit(X_train, y_train)\r\n",
    "show_metrics(model, test_df, target='target')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "show_metrics(model, train_df, target='target')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overfitting and underfitting apply in here as well"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "compare_models(DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=3), df, target='target')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "compare_models(DecisionTreeClassifier(max_depth=3), DecisionTreeClassifier(max_depth=10), df, target='target')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('bootcamp_env': venv)"
  },
  "interpreter": {
   "hash": "961b6a872976a530e9eb1b75fdcd3f6fe3550ef1322ec7de01e1a8a6f96b7246"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}