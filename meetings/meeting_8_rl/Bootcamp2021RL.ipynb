{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d150af2",
   "metadata": {},
   "source": [
    "Author: Jakub ≈Åyskawa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d024e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545c962",
   "metadata": {},
   "source": [
    "# Discrete env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c86b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d119c9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00379124,  1.4129066 ,  0.38401106,  0.08828293, -0.00438645,\n",
       "       -0.08698429,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6dbc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e997cdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c69f4a",
   "metadata": {},
   "source": [
    "# Show env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75212093",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_env.reset()\n",
    "sample_env.render()\n",
    "\n",
    "while True:\n",
    "    _, _, done, _ = sample_env.step(sample_env.action_space.sample())\n",
    "    sample_env.render()\n",
    "    if done:\n",
    "        sample_env.reset()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd719e9",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "408b0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_agent(env, agent):\n",
    "    obs = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    reward_sum = 0\n",
    "    \n",
    "    while True:\n",
    "        obs, reward, done, _ = env.step(agent.act(obs, explore=False))\n",
    "        reward_sum += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            env.reset()\n",
    "            return reward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4661fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent):\n",
    "    obs = env.reset()\n",
    "\n",
    "    reward_sum = 0\n",
    "    \n",
    "    while True:\n",
    "        obs, reward, done, _ = env.step(agent.act(obs, explore=False))\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            return reward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbcf05ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(env, outputs, neurons=32):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(env.observation_space.shape),\n",
    "        tf.keras.layers.Dense(neurons, activation='tanh'),\n",
    "        tf.keras.layers.Dense(neurons, activation='tanh'),\n",
    "        tf.keras.layers.Dense(outputs),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa24e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_I = 5000\n",
    "TEST_N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26557400",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "562b6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, env, model, gamma, exploration, lr):\n",
    "        self.env = env\n",
    "        self.obs = self.env.reset()\n",
    "\n",
    "        self.model = model\n",
    "        self.saved_model = tf.keras.models.clone_model(model)\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.exploration = exploration\n",
    "\n",
    "    def step(self):\n",
    "        action = self.act(self.obs)\n",
    "        obs, reward, done, _ = self.env.step(action)\n",
    "        prev_obs = self.obs\n",
    "\n",
    "        if done:\n",
    "            self.obs = self.env.reset()\n",
    "        else:\n",
    "            self.obs = obs\n",
    "\n",
    "        return prev_obs, obs, action, reward, done\n",
    "\n",
    "    def act(self, obs, explore=True):\n",
    "        if explore and np.random.rand() < self.exploration:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            out = self.model(obs.reshape(1, -1)).numpy()\n",
    "            return np.argmax(out)\n",
    "\n",
    "    def loss(self, obs, obs_next, actions, rewards, dones):\n",
    "        batch_size = obs.shape[0]\n",
    "\n",
    "        q_next = self.model(obs_next).numpy() * (1 - dones).reshape(batch_size, 1)\n",
    "        q = self.model(obs)\n",
    "\n",
    "        diffs = tf.gather_nd(q, actions.reshape(batch_size, 1), batch_dims=1) - (rewards + self.gamma * q_next.max(axis=-1))\n",
    "\n",
    "        return tf.reduce_sum(diffs ** 2)\n",
    "\n",
    "    def learning_step(self, obs, obs_next, actions, rewards, dones):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss(obs, obs_next, actions, rewards, dones)\n",
    "\n",
    "        v = self.model.trainable_variables\n",
    "        self.optimizer.minimize(loss, v, tape=tape)\n",
    "\n",
    "        return loss.numpy().mean()\n",
    "\n",
    "    def save_model(self):\n",
    "        self.saved_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def restore_model(self):\n",
    "        self.model.set_weights(self.saved_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "584e575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(env, make_model(env, env.action_space.n), 0.9, 0.05, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5802be63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-455.678304544497"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "679ee87c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_q_agent(env, gamma=0.9, exploration=0.05, lr=0.0001, batch_size=100, steps=100000, neurons=32):\n",
    "    train_env = gym.make(env)\n",
    "    test_env = gym.make(env)\n",
    "    agent = QAgent(train_env, make_model(train_env, train_env.action_space.n, neurons), gamma, exploration, lr)\n",
    "\n",
    "    total_loss = 0\n",
    "    t = 0\n",
    "\n",
    "    best_rewards = -np.inf\n",
    "\n",
    "    def test():\n",
    "        nonlocal best_rewards\n",
    "        reward_mean = sum(test_agent(test_env, agent) for _ in range(TEST_N)) / TEST_N\n",
    "        print(f'Step: {i + 1} mean reward sum: {reward_mean} mean loss: {total_loss / t if t else 0}')\n",
    "        if reward_mean > best_rewards:\n",
    "            agent.save_model()\n",
    "            print('Saved')\n",
    "            best_rewards = reward_mean\n",
    "\n",
    "    for i in range(steps):\n",
    "        obs, obs_next, action, reward, done = agent.step()\n",
    "\n",
    "        total_loss += agent.learning_step(\n",
    "            obs.reshape((1, -1)),\n",
    "            obs_next.reshape((1, -1)),\n",
    "            np.array(action).reshape((1, -1)),\n",
    "            np.array(reward).reshape((1,)),\n",
    "            np.array(done).reshape((1,)),\n",
    "        )\n",
    "\n",
    "        t += 1\n",
    "\n",
    "        if i % TEST_I == 0:\n",
    "            test()\n",
    "            total_loss = 0\n",
    "            t = 0\n",
    "    \n",
    "    test()\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "804ccdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 mean reward sum: -577.5624259495737 mean loss: 1.9993765354156494\n",
      "Step: 5001 mean reward sum: -132.9692685234541 mean loss: 77.90897583691327\n",
      "Step: 10001 mean reward sum: -160.18069955759742 mean loss: 48.059068123479726\n",
      "Step: 15001 mean reward sum: -437.350393322355 mean loss: 20.770620531397352\n",
      "Step: 20001 mean reward sum: -2359.1489176483856 mean loss: 22.181179281515675\n",
      "Step: 25001 mean reward sum: -1334.8973639842793 mean loss: 12.627284348996357\n",
      "Step: 30001 mean reward sum: -246.41629337096884 mean loss: 13.009079466306416\n",
      "Step: 35001 mean reward sum: -133.75057802899585 mean loss: 5.696019855153421\n",
      "Step: 40001 mean reward sum: -152.87909088970406 mean loss: 7.985132964754268\n",
      "Step: 45001 mean reward sum: -133.162337494026 mean loss: 16.87861307651841\n",
      "Step: 50001 mean reward sum: -142.32250552443446 mean loss: 7.772499423858103\n",
      "Step: 55001 mean reward sum: -72.03246179312387 mean loss: 5.992745566139879\n",
      "Step: 60001 mean reward sum: -115.32136350613139 mean loss: 3.548832415075299\n",
      "Step: 65001 mean reward sum: 0.8402238295259821 mean loss: 5.45316843088555\n",
      "Step: 70001 mean reward sum: -96.47396264821327 mean loss: 3.231677348722463\n",
      "Step: 75001 mean reward sum: -52.011507741287076 mean loss: 12.216966464911478\n",
      "Step: 80001 mean reward sum: 207.15622839127622 mean loss: 12.785079429537037\n",
      "Step: 85001 mean reward sum: 125.1760310669292 mean loss: 21.939712687653817\n",
      "Step: 90001 mean reward sum: 202.40744616154103 mean loss: 22.805109778200688\n",
      "Step: 95001 mean reward sum: 171.45110255518097 mean loss: 13.02902819880003\n",
      "Step: 100000 mean reward sum: 23.857617200478636 mean loss: 17.462730657063478\n"
     ]
    }
   ],
   "source": [
    "agent1 = train_q_agent('LunarLander-v2', gamma=0.97, lr=0.003, neurons=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79b50f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-38.85185349665383"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent(env, agent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f51be63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1.restore_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4abc0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-429.5328211961983"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent(env, agent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a03b85",
   "metadata": {},
   "source": [
    "# Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55a8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, size, observation_space, action_space):\n",
    "        self.obs = np.zeros((size,) + observation_space.shape)\n",
    "        self.obs_next = np.zeros((size,) + observation_space.shape)\n",
    "        self.actions = np.zeros((size,) + action_space.shape, dtype=action_space.dtype)\n",
    "        self.dones = np.zeros((size,))\n",
    "        self.rewards = np.zeros((size,))\n",
    "        self.probs = np.zeros((size,))\n",
    "        self.size = size\n",
    "        self.cur_size = 0\n",
    "        self.cur = 0\n",
    "\n",
    "    def put(self, obs, obs_next, action, reward, done, prob=1.):\n",
    "        self.obs[self.cur] = obs\n",
    "        self.obs_next[self.cur] = obs_next\n",
    "        self.actions[self.cur] = action\n",
    "        self.dones[self.cur] = done\n",
    "        self.rewards[self.cur] = reward\n",
    "        self.probs[self.cur] = prob\n",
    "\n",
    "        self.cur = (self.cur + 1) % self.size\n",
    "        self.cur_size = min(self.cur_size + 1, self.size)\n",
    "\n",
    "    def get(self, batch_size, probs=False):\n",
    "        ids = np.random.choice(self.cur_size, size=batch_size)\n",
    "        batch = self.obs[ids], self.obs_next[ids], self.actions[ids], self.rewards[ids], self.dones[ids]\n",
    "        return batch + ((self.probs[ids],) if probs else ())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f176bfc1",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6412ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_dqn_agent(env, gamma=0.9, exploration=0.05, lr=0.0001, batch_size=100, steps=100000, memsize=100000, neurons=32):\n",
    "    train_env = gym.make(env)\n",
    "    test_env = gym.make(env)\n",
    "    agent = QAgent(train_env, make_model(train_env, train_env.action_space.n, neurons), gamma, exploration, lr)\n",
    "\n",
    "    memory = Memory(memsize, train_env.observation_space, train_env.action_space)\n",
    "\n",
    "    total_loss = 0\n",
    "    t = 0\n",
    "    \n",
    "    best_rewards = -np.inf\n",
    "\n",
    "    def test():\n",
    "        nonlocal best_rewards\n",
    "        reward_mean = sum(test_agent(test_env, agent) for _ in range(TEST_N)) / TEST_N\n",
    "        print(f'Step: {i + 1} mean reward sum: {reward_mean} mean loss: {total_loss / t if t else 0}')\n",
    "        if reward_mean > best_rewards:\n",
    "            agent.save_model()\n",
    "            print('Saved')\n",
    "            best_rewards = reward_mean\n",
    "        \n",
    "    \n",
    "    for i in range(steps):\n",
    "        obs, obs_next, action, reward, done = agent.step()\n",
    "        memory.put(obs, obs_next, action, reward, done)\n",
    "\n",
    "        total_loss += agent.learning_step(*memory.get(batch_size))\n",
    "\n",
    "        t += 1\n",
    "\n",
    "        if i % TEST_I == 0:\n",
    "            test()\n",
    "            total_loss = 0\n",
    "            t = 0\n",
    "    \n",
    "    test()\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f837db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 mean reward sum: -589.4532259495973 mean loss: 451.6076965332031\n",
      "Saved\n",
      "Step: 5001 mean reward sum: 25.566366433121694 mean loss: 1603.343657772827\n",
      "Saved\n",
      "Step: 10001 mean reward sum: -2.6532086871345277 mean loss: 1019.8448606384277\n",
      "Step: 15001 mean reward sum: 91.62939103652526 mean loss: 786.3614880767823\n",
      "Saved\n",
      "Step: 20001 mean reward sum: 52.431662887182014 mean loss: 935.8190541107177\n",
      "Step: 25001 mean reward sum: -62.101914926346254 mean loss: 1114.9148112884523\n",
      "Step: 30001 mean reward sum: 144.5247762769173 mean loss: 1187.7598142807008\n",
      "Saved\n",
      "Step: 35001 mean reward sum: 95.73312910134895 mean loss: 1148.4959586090088\n",
      "Step: 40001 mean reward sum: 105.62987031618741 mean loss: 1078.9828122619629\n",
      "Step: 45001 mean reward sum: -56.76358648452176 mean loss: 1040.9512893493652\n",
      "Step: 50001 mean reward sum: 188.78410902172124 mean loss: 922.8347435745239\n",
      "Saved\n",
      "Step: 55001 mean reward sum: -18.72456238830589 mean loss: 931.2889545135498\n",
      "Step: 60001 mean reward sum: 159.92618162033367 mean loss: 945.9122142868042\n",
      "Step: 65001 mean reward sum: 149.11167263555265 mean loss: 978.5992528388977\n",
      "Step: 70001 mean reward sum: 26.728485009528917 mean loss: 1043.579554045868\n",
      "Step: 75001 mean reward sum: 84.1998793977024 mean loss: 1097.1713658195495\n",
      "Step: 80001 mean reward sum: 97.72374387559576 mean loss: 1033.1588693099975\n",
      "Step: 85001 mean reward sum: 209.52542868703412 mean loss: 1056.807713923645\n",
      "Saved\n",
      "Step: 90001 mean reward sum: 194.7651354136619 mean loss: 1083.2947143218994\n",
      "Step: 95001 mean reward sum: -36.6031668138354 mean loss: 1070.5647117279052\n",
      "Step: 100000 mean reward sum: -28.185088014575477 mean loss: 1038.600284725982\n"
     ]
    }
   ],
   "source": [
    "agent = train_dqn_agent('LunarLander-v2', gamma=0.97, lr=0.003, neurons=64, steps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e2ac15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.09914197710935"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea0accb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.restore_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f105714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152.80746353685973"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_agent(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19c20d",
   "metadata": {},
   "source": [
    "# Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f52e9228",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_env_c = gym.make('LunarLanderContinuous-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5857e941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env_c.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "022bbe63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1. -1.], [1. 1.], (2,), float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_env_c.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "983f331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a36edecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, env, actor, critic, gamma, sigma, actor_lr, critic_lr, beta=0.1, b=3):\n",
    "        self.env = env\n",
    "        self.obs = self.env.reset()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.b = b\n",
    "        self.beta = beta\n",
    "        self.bounds = np.maximum(self.env.action_space.low, self.env.action_space.high).reshape((1, -1))\n",
    "\n",
    "        self.noise = tfp.distributions.MultivariateNormalDiag(\n",
    "            tf.zeros(self.env.action_space.shape),\n",
    "            tf.ones(self.env.action_space.shape) * sigma\n",
    "        )\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n",
    "        \n",
    "    def step(self):\n",
    "        action, prob = self.act(self.obs.reshape(1, -1))\n",
    "        obs, reward, done, _ = self.env.step(action)\n",
    "        prev_obs = self.obs\n",
    "\n",
    "        if done:\n",
    "            self.obs = self.env.reset()\n",
    "        else:\n",
    "            self.obs = obs\n",
    "\n",
    "        return prev_obs, obs, action, reward, done, prob\n",
    "\n",
    "    def act(self, obs, explore=True):\n",
    "        means = self.actor(obs.reshape((1, -1))).numpy()[0]\n",
    "        \n",
    "        if explore:\n",
    "            noise = self.noise.sample()\n",
    "            probs = self.noise.prob(noise)\n",
    "            return means + noise.numpy(), probs.numpy()\n",
    "        else:\n",
    "            return means\n",
    "\n",
    "    def learning_step(self, obs, obs_next, actions, rewards, dones, probs):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            v = self.critic(obs)\n",
    "            v_next = tf.stop_gradient(self.critic(obs_next)) * (1 - dones).reshape((-1, 1))\n",
    "            cur_actions = self.actor(obs)\n",
    "            td = (rewards.reshape((-1, 1)) + self.gamma * v_next) - v\n",
    "            \n",
    "            cur_probs = self.noise.prob(actions - cur_actions)\n",
    "\n",
    "            is_base = tf.reshape((cur_probs / probs), (-1, 1))\n",
    "            is_ = np.tanh(is_base / self.b) * self.b\n",
    "\n",
    "            critic_loss = tf.reduce_mean(td ** 2 * tf.stop_gradient(is_))\n",
    "            actor_loss = tf.reduce_mean(-self.noise.log_prob(actions - cur_actions) * tf.stop_gradient(td * is_))\n",
    "\n",
    "            bounds_penalty = tf.reduce_mean(tf.maximum(tf.abs(cur_actions) - self.bounds, 0.) ** 2 * self.beta)\n",
    "\n",
    "            actor_loss_with_bounds = actor_loss + bounds_penalty\n",
    "\n",
    "        self.actor_optimizer.minimize(actor_loss_with_bounds, self.actor.trainable_variables, tape=tape)\n",
    "        self.critic_optimizer.minimize(critic_loss, self.critic.trainable_variables, tape=tape)\n",
    "        \n",
    "        return np.array([critic_loss.numpy(), actor_loss.numpy(), bounds_penalty.numpy()])\n",
    "\n",
    "    def save_model(self):\n",
    "        self.saved_actor = tf.keras.models.clone_model(self.actor)\n",
    "        self.saved_critic = tf.keras.models.clone_model(self.critic)\n",
    "\n",
    "    def restore_model(self):\n",
    "        self.actor = tf.keras.models.clone_model(self.saved_actor)\n",
    "        self.critic = tf.keras.models.clone_model(self.saved_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66f595fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic_agent(\n",
    "    env, \n",
    "    gamma=0.9, sigma=0.25, actor_lr=0.00001, critic_lr=0.00001, \n",
    "    batch_size=100, steps=100000, memsize=100000, learning_starts=1000, neurons=64):\n",
    "    train_env = gym.make(env)\n",
    "    test_env = gym.make(env)\n",
    "    agent = ActorCriticAgent(\n",
    "        train_env,\n",
    "        make_model(train_env, train_env.action_space.shape[0], neurons),\n",
    "        make_model(train_env, 1, neurons),\n",
    "        gamma,\n",
    "        sigma,\n",
    "        actor_lr,\n",
    "        critic_lr\n",
    "    )\n",
    "\n",
    "    memory = Memory(memsize, train_env.observation_space, train_env.action_space)\n",
    "\n",
    "    total_loss = 0\n",
    "    t = 0\n",
    "    \n",
    "    best_rewards = -np.inf\n",
    "\n",
    "    def test():\n",
    "        nonlocal best_rewards\n",
    "        reward_mean = sum(test_agent(test_env, agent) for _ in range(TEST_N)) / TEST_N\n",
    "        print(f'Step: {i + 1} mean reward sum: {reward_mean} mean loss: {total_loss / t if t else 0}')\n",
    "        if reward_mean > best_rewards:\n",
    "            agent.save_model()\n",
    "            print('Saved')\n",
    "            best_rewards = reward_mean\n",
    "        \n",
    "    \n",
    "    for i in range(steps):\n",
    "        obs, obs_next, action, reward, done, prob = agent.step()\n",
    "        memory.put(obs, obs_next, action, reward, done, prob)\n",
    "\n",
    "        if i >= learning_starts:\n",
    "            total_loss += agent.learning_step(*memory.get(batch_size, True))\n",
    "\n",
    "        t += 1\n",
    "\n",
    "        if i % TEST_I == 0:\n",
    "            test()\n",
    "            total_loss = 0\n",
    "            t = 0\n",
    "    \n",
    "    test()\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9576d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 mean reward sum: -401.41748249522874 mean loss: 0.0\n",
      "Saved\n",
      "Step: 5001 mean reward sum: -679.6874770824703 mean loss: [   1.075956 -549.7446    242.49194 ]\n",
      "Step: 10001 mean reward sum: -157.08632907158258 mean loss: [ 5.3108168e-01 -7.7965417e+02  3.5481546e+02]\n",
      "Saved\n",
      "Step: 15001 mean reward sum: -416.2332664487665 mean loss: [  1.4009084 -20.140121    1.473731 ]\n",
      "Step: 20001 mean reward sum: -427.3849108320004 mean loss: [  1.4612473 -33.61339     1.1907741]\n",
      "Step: 25001 mean reward sum: -403.70088146613335 mean loss: [  1.764594  -29.53783     1.8621186]\n",
      "Step: 30001 mean reward sum: -241.80583687378072 mean loss: [  1.6304636 -28.923712    1.8546697]\n",
      "Step: 35001 mean reward sum: -108.74096714086495 mean loss: [ 2.9188602 -5.2435503  1.555732 ]\n",
      "Saved\n",
      "Step: 40001 mean reward sum: -530.2717268279341 mean loss: [  2.1983721 -13.903837    1.28333  ]\n",
      "Step: 45001 mean reward sum: -1044.5089603356525 mean loss: [  2.572347  -10.231569    1.3730661]\n",
      "Step: 50001 mean reward sum: -150.80501107401113 mean loss: [  2.4461715 -15.847539    5.4341097]\n"
     ]
    }
   ],
   "source": [
    "train_actor_critic_agent('LunarLanderContinuous-v2', gamma=0.97, actor_lr=0.001, critic_lr=0.001, sigma=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actor_critic_agent('LunarLanderContinuous-v2', gamma=0.97, actor_lr=0.0001, critic_lr=0.0001, sigma=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actor_critic_agent('LunarLanderContinuous-v2', gamma=0.97, actor_lr=0.01, critic_lr=0.01, sigma=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actor_critic_agent('LunarLanderContinuous-v2', gamma=0.97, actor_lr=0.001, critic_lr=0.001, sigma=0.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
