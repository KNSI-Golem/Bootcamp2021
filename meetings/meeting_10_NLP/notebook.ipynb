{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Czym wgl jest NLP?\n",
    "![Czym jest NLP](https://7wdata.be/wp-content/uploads/2021/09/DCF_NLP-in-the-Data-Center_ML-DL-Diagram.png)\n",
    "\n",
    "NLP - Natural Language Processing to dział AI zajmujący się przetwarzaniem języka naturalnego. Czyli takie rzeczy jak:\n",
    "* Generowanie artykułów\n",
    "* Podsumowywanie tekstu\n",
    "* POS (part of speach tagging)\n",
    "* Przetwarzanie zbiorów danych\n",
    "* Automatyczne Question anwsering\n",
    "itp... Ogólnie wszystko powiązanego z językami, którymi się posługują ludzie.\n",
    "Na codzień mamy doczynienia z tekstem więc nic dziwnego, że jest to tak istotna i prężnie rozwijająca się działka AI.\n",
    "W związku z tym że korzystamy z komputerów pojawiają się problemy jak reprezentować ten tekst, jak go przetwarzać."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jak reprezentować tekst - podstawowe sposoby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHotEncoding\n",
    "Najprostszym sposobem jest już znany wam pewnie OneHotEncoding. Robimy to w następujący sposób:\n",
    "1) tworzymy słownik ze słowami zawierającymi się w naszym zbiorze $V=(słowo_1,słowo_2,\\dots,słowo_k)$\n",
    "2) reprezentujemy dany tekst na podstawie wektora 0 i 1, gdzie 0 dajemy jeżeli wystąpiło dane słowo\n",
    "czyli \n",
    "$$T = (a_1,a_2,\\dots,a_{|V|}) \\quad \\text{gdzie } a_i =\\begin{cases} 0 \\text{ gdy }słowo_i\\not\\in Tekst\\\\  1 \\text{ gdy }słowo_i\\in Tekst\\end{cases}$$\n",
    "Gdzie V to nasz słownik, a T to reprezentacja.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(text):\n",
    "    words = text.split()\n",
    "    vocab = list(set(words))\n",
    "    return vocab\n",
    "\n",
    "def encode_text(text, vocab):\n",
    "    text_words = set(text.split())\n",
    "    return [word in text_words for word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ala ma psa\"\n",
    "text2 = \"Maciek nie ma psa\"\n",
    "\n",
    "vocab = create_vocab(text)\n",
    "encode_text(text2, vocab), vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "Wadą naszego porzedniego rozwiązania jest to, że tylko sprawdza czy słowo wystąpiło, dlatego teraz dodatkowo będziemy zliczali liczbę wystąpień słów.\n",
    "1) tworzymy słownik ze słowami zawierającymi się w naszym zbiorze $V=(słowo_1,słowo_2,\\dots,słowo_k)$\n",
    "2) reprezentujemy dany tekst na podstawie wektora wystąpień słów ze słownika w tekście\n",
    "czyli \n",
    "$$T = (a_1,a_2,\\dots,a_{|V|}) \\quad \\text{gdzie } a_i-\\text{liczba wystąpień }słowa_i\\text{ w tekście}$$\n",
    "Gdzie V to nasz słownik, a T to reprezentacja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer().fit(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = vectorizer.transform(twenty_train.data[:1])\n",
    "transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer().fit(['Ala ma psa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = vectorizer.transform(['Maciek ma psa i ma wiele kotów, które nie lubią psów'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.toarray()\n",
    "# Zauważcie pierwszy problem, psów i psy mają tak naprawdę to samo znaczenie ale przez to\n",
    "# że są w innej odmianie nie są zliczane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency Inverse Document Frequency)\n",
    "Zauważmy, że wadą zliczania jest to, że mogą występować w naszym zbiorze teksty, w których często powtarza się\n",
    "to samo słowo i wtedy to że np. słowo \"paragraf\" wystąpiło 2137 za dużo nie znaczy. Z pomocą nadchodzi TF-IDF, który waży liczności\n",
    "w zależności od tego w ilu dokumentach dane słowo wystąpiło, czyli większe wagi chcemy przywiązywać słowom \"rozróżniającym\". Niech t oznacza token, d oznacza dokument oraz D-zbiór dokumentów wtedy\n",
    "\\begin{align*}\n",
    "    &tf(t,d)=\\frac{f_{t,d}}{\\sum_{t'\\in d} f_{t',d}}\\\\\n",
    "    &idf(t,D)=\\log\\frac{N}{|\\{d\\in D;\\,t\\in d\\}}\\\\\n",
    "    &tfidf(t,d,D)=tf(t,d)\\times idf(t,D)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer().fit(twenty_train.data)\n",
    "transformed_data = vectorizer.transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge!\n",
    "Korzystając z przedstawionych wcześniej reprezentacji tekstu przeucz drzewo losowe przewidujące klasę newsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
    "train_text, train_class = twenty_train.data, twenty_train.target\n",
    "test_text, test_class = twenty_test.data, twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rozwiązanie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gramy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy, że czasem istotną informacją mogą być ciągi słów. Na tym właśnie polegają n-gramy. N-gram jest to ciąg n-słów.\n",
    "\n",
    "Przykład 2-grama: \\[\"Ala\", \"ma\"\\], \\[\"ma\", \"Psa\"\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funckja ngrams wymaga podania na wejście sekwencji\n",
    "list(ngrams(text.split(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wszystkie wcześniej wymienione sposoby reprezentacji tekstu domyślnie operują na słowach ale mogą operować też na n-gramach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2)).fit(['Ala ma psa'])\n",
    "transformed = vectorizer.transform(['Maciek ma psa i ma wiele kotów, które nie lubią psów'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogicznie dla TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wracamy do poprzedniego zadania, spróbujcie poprawić wynik wykorzystując n-gramy, tylko nie przesadzajcie z n bo wam nie starczy RAMu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
    "train_text, train_class = twenty_train.data, twenty_train.target\n",
    "test_text, test_class = twenty_test.data, twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozwiązanie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zmniejszanie słownika\n",
    "### Stop words\n",
    "W każdym języku znajdują się słowa, które same nie niosą żadnej informacji, czasami warto pozbyć się takich słów, aby zmniejszyć wymiar słownika itp.\n",
    "\n",
    "Przykłady dla angielskiego: \"The\" \"who\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming i Lematyzacja\n",
    "\n",
    "Jak wcześniej zwróciłem uwagę czasami nie interesuje nas odmiana słowa tylko czy samo słowo wystąpiło. Np. chcemy w tekście szukać czy wystąpiła jakakolwiek odmiana słowa \"pies\" wtedy dokonujemy stemmingu(usunięcia ostatnich znaków ze słowa aby sprowadzić je do podstawowej formy) lub lematyzaji wykorzystującej kontekst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"caresses\",\n",
    "    \"flies\",\n",
    "    \"dies\",\n",
    "    \"mules\",\n",
    "    \"denied\",\n",
    "    \"died\",\n",
    "    \"agreed\",\n",
    "    \"owned\",\n",
    "    \"humbled\",\n",
    "    \"sized\",\n",
    "    \"meeting\",\n",
    "    \"stating\",\n",
    "    \"siezing\",\n",
    "    \"itemization\",\n",
    "    \"sensational\",\n",
    "    \"traditional\",\n",
    "    \"reference\",\n",
    "    \"colonizer\",\n",
    "    \"plotted\",\n",
    "]\n",
    "for word in words:\n",
    "    lemmatized, stemmed = lemmatizer.lemmatize(word), stemmer.stem(word)\n",
    "    print(f\"{word}\\nLemma:{lemmatized} Stemmed:{stemmed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex\n",
    "\n",
    "regex czyli regular expressions to narzędzie pozwalające tworzyć wzorce tekstowe, które potem można wykorzystać np. do usuwania konkretnych wzorców z tekstu w celu filtorwania, wyciąganie emaili z tekstu itd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tak się tworzy regexa\n",
    "pattern = re.compile(r'@gmail.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern.findall('kasjhfgasil jfalfhjas lfhaklfj pw@gmail.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak tworzyć regexy\n",
    "korzystając z cheat-sheetów\n",
    "\n",
    "![regex_cheat_sheet](https://media.cheatography.com/storage/thumb/davechild_regular-expressions.750.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteka re pozwala na wiele rzeczy [dokumentacja](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "funkcje:\n",
    "* __findall__: znajduje wszystkie wystąpienia wzorca\n",
    "* __search__: zwraca None albo pierwsze wystąpienie wzorca\n",
    "* __split__\n",
    "* __sub__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadanie: stworzyć regex który wyciąga emaile z tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'')\n",
    "to_search = \"asoihfjaos hoasuf oajioah ofijoe pw@edu.pl klajshflakshfjklaj lfhasnoirhj weaoijpoa m abkcp@onet.pl\"\n",
    "pattern.findall(to_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### grupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_with_group = r'(\\w+)@[\\w\\.]+'\n",
    "re.findall(regex_with_group, to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_with_group = r'(\\w+)@([\\w\\.]+)'\n",
    "re.findall(regex_with_group, to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(regex_with_group, \"\\\\1@uganda.ug\", to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(regex_with_group, \"ale_jazda@\\\\2\", to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_with_group = r'(\\w+)@(?:[\\w\\.]+)'\n",
    "re.findall(regex_with_group, to_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizacja\n",
    "\n",
    "Poważnym problemem jest w jaki sposób tworzyć tokeny z tekstu:\n",
    "* znaki oddzielone spacjami?\n",
    "* znaki oddzielone na znakach interpunkcyjnych?\n",
    "* rozdzielanie na podstawie regexów?\n",
    "* inne metody?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oddzielanie spacjami\n",
    "jest to najprostszy sposób, ale też nie najlepszy, ponieważ np. słowo \"won't\" będzie jednym tokenem a samo \"n't\" można też dodać jako oddzielny token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### oddzielanie na znakach interpunkcyjnych i spacjach\n",
    "Tutaj dodatkowo rozdzielamy spacją wszystkie znaki interpunkcyjne czyli np. \"Ala ma psa.\" -> \\[\"Ala\", \"ma\", \"psa\", \".\"\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### oddzielanie na regexach\n",
    "jest to połączenie poprzednich metod i dodanie specjalnych fraz np. oddzielanie \"n't\" w języku angielskim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(\"I didn't want to come.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subwords tokenization\n",
    "Wszystkie poprzednie metody mają problem z nowymi słowami, które mogą się pojawić podczas tokenizacji nowego tekstu. W przypadku poprzednich metod zastępuje się zazwyczaj słowa nie występujące w słowniku przez \"<unk>\". \n",
    "\n",
    "Kolejnym problemem jest wielkość słownika, im więcej słów chcemy posiadać tym większy musi być nasz słownik co prowadzi do coraz większych wymagań pamięciowych w celu operowania na tekstach. \n",
    "\n",
    "Problemy te są rozwiązywane przez tokenizatory, który dokonują podziału na tokeny, które nie są całymi słowami tylko ich fragmentami. W takich modelach z góry określa się wielkość słownika. Oczywiście powstaje pytanie jak wybierać ciągu znaków, które będą tokenami.\n",
    "Przykłady algorytmów\n",
    "* [Byte-Pair Encoding](https://arxiv.org/abs/1508.07909)\n",
    "* [WordPiece](https://ai.googleblog.com/2021/12/a-fast-wordpiece-tokenization-system.html)\n",
    "* [Unigram Language Model](https://arxiv.org/pdf/1804.10959.pdf)\n",
    "* [SentencePiece](https://jacky2wong.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)\n",
    "\n",
    "Warto na nie zwrócić uwagę bo wszystkie aktualnie najlepsze modele językowe oparte o sieci neuronowe z nich korzystają ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"I have a new GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=2022, min_frequency=1)\n",
    "\n",
    "with open(\"tokenizer_train.txt\", 'w') as f:\n",
    "    for line in twenty_train.data[:100]:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(['tokenizer_train.txt'], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"Czas na tokenizacje.\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z biblioteką [tokenizers](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#) dodać wiele różnych rzeczy jak dodawanie specjalnych tokenów na początek koniec, wiele innych pretokenizatorów i wiele innych gotowych tokenizatorów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy i problemy lingwistyczne\n",
    "spacy jest biblioteką zawierającą wiele modeli do problemów lingwistycznych, które teraz sobie krótko omówimy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tokenizacja\n",
    "spacy także posiada tokenizacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wczytywanie modelu językowego\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Akash has been buyed by byju's in 73,000 Core's\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-Of_Speech (POS) - tagging\n",
    "problem ten polega na wyjaśnieniu w jaki sposób dane słowo jest wykorzystane w zdaniu. Ustalone jest 8 części mowy (po ang bo będzie łatwiej):\n",
    "* Noun\n",
    "* Pronoun\n",
    "* Adjective\n",
    "* Verb\n",
    "* Adverb\n",
    "* Preposition\n",
    "* Conjunction\n",
    "* Interjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I am Ritesh,currently a Computer Science and NLP Researcher\")\n",
    " \n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Print the token and its part-of-speech tag\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I am Ritesh,currently a Computer Science and NLP Researcher\")\n",
    "spacy.displacy.render(doc, style=\"dep\" , jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependency Parsing\n",
    "Jest to proces tworzenia struktury gramatycznej zdania. Daje on nam zależność słów w zdaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I am Ritesh,currently a Computer Science and NLP Researcher\")\n",
    " \n",
    "for token in doc:\n",
    "    print(token.text, \"-->\", token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recogniton\n",
    "Czyli po prostu wykrywanie nazw własnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Reliance is looking at buying U.K. based analytics startup for $7 billion\")\n",
    "#See the entity present\n",
    "print(f\"Enitites: {doc.ents}\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity Recogniton\n",
    "nazywane też Entity Detection jest bardziej zaawansowany od NER, ponieważ rozpoznaje istotne elementy między innymi miejsca, ludzi, organizacje, języki itp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc= nlp(u\"\"\"The Amazon rainforest,[a] alternatively, the Amazon Jungle, also known in English as Amazonia, is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations.\n",
    "\n",
    "The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Bolivia, Ecuador, French Guiana, Guyana, Suriname, and Venezuela. Four nations have \"Amazonas\" as the name of one of their first-level administrative regions and France uses the name \"Guiana Amazonian Park\" for its rainforest protected area. The Amazon represents over half of the planet's remaining rainforests,[2] and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.[3]\n",
    "\n",
    "Etymology\n",
    "The name Amazon is said to arise from a war Francisco de Orellana fought with the Tapuyas and other tribes. The women of the tribe fought alongside the men, as was their custom.[4] Orellana derived the name Amazonas from the Amazons of Greek mythology, described by Herodotus and Diodorus.[4]\n",
    "\n",
    "History\n",
    "See also: History of South America § Amazon, and Amazon River § History\n",
    "Tribal societies are well capable of escalation to all-out wars between tribes. Thus, in the Amazonas, there was perpetual animosity between the neighboring tribes of the Jivaro. Several tribes of the Jivaroan group, including the Shuar, practised headhunting for trophies and headshrinking.[5] The accounts of missionaries to the area in the borderlands between Brazil and Venezuela have recounted constant infighting in the Yanomami tribes. More than a third of the Yanomamo males, on average, died from warfare.[6]\"\"\")\n",
    "\n",
    "entities=[(i, i.label_, i.label) for i in doc.ents]\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style = \"ent\",jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprezentacja słów i sieci neuronowe czyli embeddingi\n",
    "Embeddingi polegają na zmniejszeniu wymiaru danych tekstowych, aby zakodować ciąg słów o słowniku wielkości 50000 tworzymy macierz o wymiarach seq_len x 50000, co można się domyślić nie jest optymalne, embeddingi sprowadzają dane tekstowe do dużo niższego wymiaru np. 300.\n",
    "\n",
    "Przykładami embeddingów są:\n",
    "* [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    "* [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "* [FastText](https://fasttext.cc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "![Word2vec image](https://imgs.developpaper.com/imgs/1091794672-5c7cc60fab3ba_articlex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podobieństwo słów\n",
    "Mając embeddingi słów można badać ich podobieństwo badając ich odległość w przestrzeni, w której się znajdują. Najczęściej wykorzystuje się do tego odległość cosinusową\n",
    "\\begin{align*}\n",
    "    cos\\_sim(A,B) = \\frac{A\\cdot B}{||A||||B||}=cos(\\theta)\n",
    "\\end{align*}\n",
    "która pokazuje jaki jest kąt między dwoma wektorami, jeżeli 0 wtedy mamy 1 i oznacza to że wektory są tak samo skierowane czyli są podobne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence', 'the', 'is', 'this', 'final', 'and', 'more', 'one', 'another', 'yet', 'second', 'word2vec', 'for', 'first']\n",
      "[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "# size: (default 100) wymiar przestrzeni embeddingów.\n",
    "# window: (default 5) okno które będzie wykorzystywane do predykcji lub będzie predykowane.\n",
    "# min_count: (default 5) minimalna liczba wystąpień słowa aby było uwzględnione w słowniku.\n",
    "# workers: (default 3) liczba wątków wykorzystana do uczenia.\n",
    "# sg: (default 0 or CBOW) jaki algorytm ma być wykorzystany do uczenia 0-CBOW, 1-Skip-gram.\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "words = list(model.wv.key_to_index.keys())\n",
    "print(words)\n",
    "print(model.wv['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model.wv[words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transformed = PCA(2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_transformed[:, 0], x_transformed[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "\tplt.annotate(word, xy=(x_transformed[i, 0], x_transformed[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f60acc4feb66875ce3d3a7c45fd7be20da61778b03efde0a9ca4c6b3f9535b2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('bootcamp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
